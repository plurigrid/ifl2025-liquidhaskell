\documentclass[sigconf, anonymous, review]{acmart}
\citestyle{acmnumeric}
\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage[store-all]{keytheorems}

%%%%%%%%%%%%%%%%% Editing marks %%%%%%%%%%%%%%%%%

  % TOGGLE ME to turn off all the commentary:
  \InputIfFileExists{no-editing-marks}{
    \def\noeditingmarks{}
  }

  \usepackage{xargs}
  \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
  % ^^ Need for pgfsyspdfmark apparently?
  \ifx\noeditingmarks\undefined
      % Adapting to acmart's small margins
      \setlength{\marginparsep}{0.3em}
      \setlength{\marginparwidth}{1.4cm}

      \newcommandx{\unsure}[2][1=]{\todo[linecolor=orange,backgroundcolor=orange!25,bordercolor=orange,#1]{#2}}
      \newcommandx{\unsureF}[2][1=]{\todo[linecolor=orange,backgroundcolor=lightgray!25,bordercolor=orange,#1]{#2}}
      \newcommandx{\info}[2][1=]{\todo[linecolor=green,backgroundcolor=green!25,bordercolor=green,#1]{#2}}
      \newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
      \newcommandx{\inconsistent}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
      \newcommandx{\critical}[2][1=]{\todo[linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,#1]{#2}}
      \newcommand{\improvement}[1]{\todo[linecolor=pink,backgroundcolor=pink!25,bordercolor=pink]{#1}}
      \newcommandx{\resolved}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}} % use this to mark a resolved question
    \else
      \renewcommand{\todo}{}
      \newcommandx{\unsure}[2][1=]{{}}
      \newcommandx{\unsureF}[2][1=]{{}}
      \newcommandx{\info}[2][1=]{{}}
      \newcommandx{\change}[2][1=]{{}}
      \newcommandx{\inconsistent}[2][1=]{{}}
      \newcommandx{\critical}[2]{{}}
      \newcommand{\improvement}[1]{{}}
      \newcommandx{\resolved}[2][1=]{{}}

  \fi

%%%%%%%%%%%%%%%%% /Editing marks %%%%%%%%%%%%%%%%%

\title{Refinement-Types Driven Development: A study}

\author{undisclosed author/s}
\author{Facundo Dom√≠nguez}
\affiliation{
     \institution{Tweag}
     \country{Uruguay}
}
\email{facundo.dominguez@tweag.io}
\author{Arnaud Spiwack}
\affiliation{
     \institution{Tweag}
     \country{France}
}
\email{arnaud.spiwack@tweag.io}
\keywords{refinement types, Liquid Haskell, SMT solvers, program design}
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10010940.10010992.10010998.10010999</concept_id>
       <concept_desc>Software and its engineering~Software verification</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10010940.10010992.10010998.10011000</concept_id>
       <concept_desc>Software and its engineering~Automated static analysis</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011074.10011099.10011692</concept_id>
       <concept_desc>Software and its engineering~Formal software verification</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10010124.10010138.10010142</concept_id>
       <concept_desc>Theory of computation~Program verification</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10010124.10010138.10010143</concept_id>
       <concept_desc>Theory of computation~Program analysis</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software verification}
\ccsdesc[500]{Software and its engineering~Automated static analysis}
\ccsdesc[500]{Software and its engineering~Formal software verification}
\ccsdesc[300]{Theory of computation~Program verification}
\ccsdesc[300]{Theory of computation~Program analysis}


\newcommand{\tc}[1]{{\small\texttt{#1}}}
\newcommand{\codeblocksize}{\fontsize{6.5}{9}\selectfont}
\newcommand{\sourcefile}[1]{\tc{#1} in Appendix~\ref{#1}}
\RecustomVerbatimEnvironment{verbatim}{Verbatim}{
    fontsize=\codeblocksize,
}
\lstset{
    basicstyle=\ttfamily\fontsize{6}{8}\selectfont,
    breaklines=true,
    % settings to get rid of spaces when copy and pasting
    keepspaces=true,
    basewidth=0.5em,
    columns=fixed,
}
\newtheorem{principle}{Principle}

\begin{document}
\begin{abstract}
    This paper advocates for the broader application of SMT solvers in everyday
    programming, challenging the conventional wisdom that these tools are
    solely for formal methods and verification. We claim that SMT solvers, when
    seamlessly integrated into a compiler's static checks, significantly
    enhance the capabilities of ordinary type checkers in program composition.
    Specifically, we argue that refinement types, as embodied by Liquid Haskell,
    enable the use of SMT solvers in mundane programming tasks.

    Through a case study on handling binder scopes in compilers, we envision a
    future where ordinary programming is made simpler and more enjoyable with the
    aid of refinement types and SMT solvers. As a secondary contribution, we
    present a prototype implementation of a theory of finite maps for Liquid
    Haskell's solver, developed to support our case study.
\end{abstract}
\maketitle

\section{Introduction}

SMT solvers are useful to the ordinary activity of
programming. This is what we would like to convince the reader of. More
precisely, our claim is that an SMT solver well-integrated in the static checks
of a compiler, complements an ordinary type checker when composing programs.

The experience of programming with types is that of collaborating with the
compiler to write the programs we want to write. SMT solvers can be used much
the same.

SMT solvers, when it comes to their application to programming, are usually
paired with terms like ``formal methods'' or ``verification'' in the
literature~\cite{barnett05,demoura08,zinzin17,swamy22,filli13,leino17}. We would like to
challenge the wisdom that we reach for SMT-solver-based tools when we need
formal methods. We would benefit from using SMT solvers in mundane programs.

We will be arguing, in particular, that refinement types, in the guise of Liquid
Haskell~\cite{vazou14b}, let you do just that. Even though Liquid
Haskell is also usually invoked together with phrases like ``formal methods'' or
``verification''~\cite{vazou14,lehmann21,liu20,redmond23}. We would like to
challenge the wisdom that Liquid Haskell is only for formal method as well.

Through a case study, we will argue for a future where programming, ordinary
programming, is made easier and more pleasant thanks to refinements types and
SMT solvers, even though the technology isn't really ready yet. Our case study
will be the handling of binders' scopes in compilers. We distill from the experience
a set of principles that summarize concretely the impact on the programming activity.
A secondary contribution
is a prototype implementation of a theory of finite maps for Liquid Haskell's
solver, to support our case study, and which we discuss in
Section~\ref{extending-liquid-haskell}.

\section{Capture-avoiding substitutions}
\label{capture-avoiding-substitution}

Binding scope management is recognized as a persistent annoyance when writing compilers.
It's easy to get wrong and is a source of mistake to the point that many have
proposed disciplines to prevent mismanagement of scopes, like name capture.
The poster child is substitution, like in
$(\lambda x. y)[y:=t]$. The result of this substitution is $\lambda x. t$.
Thus $(\lambda x. y)[y:=x]$ is $\lambda x. x$. An easy mistake!

Compiler authors have proposed many disciplines to help make scope more
manageable.
The GHC Haskell compiler, for instance, uses an approach to avoid name capture called
\textit{the rapier}~\cite{peytonjones02secrets}. All term-manipulating functions
carry an additional \textit{scope} set containing all the
variables that appear free in its arguments. This set is
used both to decide what to rename a binder to, in order to avoid name capture,
and it is also used to skip renaming a binder if it wouldn't capture any free
variables. Figure~\ref{rapier-style-substitution} shows an implementation of
substitution
for the untyped lambda calculus.

\begin{figure}
\begin{verbatim}
data Exp = Var Int | App Exp Exp | Lam Int Exp

substitute :: Set Int -> Subst Exp -> Exp -> Exp
substitute scope s e0 = case e0 of
  Var i -> lookupSubst s i
  App e0 e1 -> App (substitute scope s e0) (substitute scope s e1)
  Lam i e
    | member i scope,
      let j = freshVar scope ->
        Lam j $ substitute (insert j scope) (extendSubst s i (Var j)) e
    | otherwise ->
        Lam i $ substitute (insert i scope) (extendSubst s i (Var i)) e

freshVar :: Set Int -> Int
freshVar s = case lookupMax s of Nothing -> 0; Just i -> i + 1
\end{verbatim}
\caption{Rapier style substitution}
\Description{Haskell implementation of substitution for untyped lambda terms using the rapier}
\label{rapier-style-substitution}
\end{figure}

\subsection{The foil}
\label{the-rapier-with-stronger-types}

The rapier wasn't enough, however, for \citet{maclaurin23} who report that
despite using the rapier they struggled with frequent scope issues in their
compiler. They set out to enforce the scope properties of the rapier with
Haskell's type system. A stunt that has often been attempted, but
\cite{maclaurin23}'s approach, that they name \emph{the foil}, is probably the
first to succeed at enforcing such invariants without incurring an unreasonable
amount of boilerplate.

Here is our distillation of the properties that \citeauthor{maclaurin23} set
out to guarantee (see also \cite[Section~4]{maclaurin23}):
\begin{enumerate}
\item Every traversed binder must be added to the scope set, lest their name
      is later used instead of a fresh name.
\item \label{req:always-rename} Every traversed binder must be renamed if it's already a member of the
      scope set, because this name could otherwise be captured as above.
\item When renaming a binder, the new name must not belong to the scope set.
\item When renaming a binder, the occurrences of the old bound variable need
      to be substituted with the new name.
\item The initial scope set must contain the free variable of all the relevant
      arguments.
% \item The binders need to be removed from the domain of the substitution when
%       they are not in the scope set, otherwise the substitution will happily
%       replace occurrences of those bound variables! An alternative we use in
%       Figure~\ref{rapier-style-substitution} is to redefine the substitution to
%       map the variable of the binder to itself.
\end{enumerate}

\Citeauthor{maclaurin23} propose a library with types \tc{Scope n}, \tc{Name n}, and
\tc{Name\-Binder n l}. A value of type \tc{Scope n} is a set of names, where
the type index \tc{n} is the name of the set at the type level. A value of type \tc{Name n} is a name that
belongs to the set with type \tc{Scope n}. A value of type \tc{NameBinder n l} is
a name in the set with type \tc{Scope l} which results from adding such single
name to the set with type \tc{Scope n}. These types are to be used in
the abstract syntax tree of terms:

\begin{quotation}
\begin{verbatim}
data Exp n = Var (Name n)
           | App (Exp n) (Exp n)
           | forall l. Lam (NameBinder n l) (Exp l)
\end{verbatim}
\end{quotation}

Then the operations and type checking on the new types will guide the user into
respecting the scope requirements when implementing substitution.

\begin{verbatim}
substitute :: Distinct o => Scope o -> Subst Expr i o -> Expr i -> Expr o
\end{verbatim}

This type signature says that no names shadow each other in the scope set \tc{o}.
It also says that the substitution will take a expression with free variables in
a scope set \tc{i} and produce an expression with free variables in a scope set
\tc{o}.

There
are mechanisms to check that a scope set is a subset of another, to assert that no
name shadows another one in a given scope set, to reason that expressions
with free variables in one scope (\tc{Exp n}) can be coerced to expressions with
free variables in a superset (\tc{Exp l}), and to introduce scope sets that extend
others with freshly created names. They also provide an implementation of maps of
variables to expressions, that is the substitutions to apply, with an interface
that uses the new types as well. There is for instance the following function to
produce fresh variables:

\begin{verbatim}
withRefreshed
  :: Distinct o
  => Scope o
  -> Name i
  -> (forall (o' :: S). DExt o o' => NameBinder o o' -> r)
  -> r
\end{verbatim}

Using the constraint \tc{DExt}, this type signature says that scope set \tc{o'}
extends the scope set \tc{o} with the given \tc{NameBinder o o'}. This binder
may have the same name as the provided \tc{Name i} if it was not present in
\tc{o}, otherwise it will be a fresh name. As another example, the following
function always produces a fresh name.

\begin{verbatim}
withFresh
  :: Distinct n
  => Scope n
  -> (forall l . DExt n l => NameBinder n l -> r )
  -> r
\end{verbatim}

With ingenious engineering and design, the foil meets its rather ambitious goal.
But it is unfortunate that the authors needed to be ingenious. All things equal,
we prefer program components to be straightforward. Because ingenious solutions
take time, and because straightforward solutions are easier to adapt when the
parameters of the problem evolve.


\subsection{A Liquid Haskell primer}

We will turn next to Liquid Haskell as our proposed solution, but first let us
introduce Liquid Haskell briefly.
Liquid Haskell is a
plugin for Haskell which statically checks that programs respect signatures
provided by the programmer. There are two key differences between Liquid Haskell
signature checking and a classical type checker:

\begin{itemize}
  \item The checking process consists in generating logical constraints or proof
        obligations which are then fed to an SMT solver, leveraging the powerful
        capabilities of SMT solvers to reason about numbers, arrays, strings, etc‚Ä¶
  \item Signatures are expressed with \emph{refinement types} of the form
        \tc{\{x:b | p\}}, which denote values of type
        base type \tc{b} that satisfy predicate \tc{p}.
        Refinements are subject to subtyping in the same way as subsets in set
        theory, so that we have
\begin{verbatim}
{-@ f :: {x:Int | x > 1} -> {x:Int | x > 0} @-}
f :: Int -> Int
f x = x
\end{verbatim}
\end{itemize}

Liquid Haskell reads refinement type signatures and other annotations from
inside such special Haskell comments
\tc{\{-@ \ldots\ @-\}}. We will skip them in our snippets when it is unambiguous.

The predicates in the refinement types are in a language of expressions
referred to as the logic language. For the sake of this paper, we can
regard it as a subset of Haskell, except that predicates are assembled both from
regular Haskell functions and functions that are
only available in the logic language.

A function like \tc{member}, which comes from the module \tc{Data.Set}
in the \tc{containers} package, is linked by Liquid Haskell to the
SMT solver's theory of sets.
\begin{verbatim}
import Data.Set
assume member :: Ord a
              => x:a -> xs:(Set a) -> {v:Bool | v <=> Set_mem x xs}
\end{verbatim}
Refinement type signatures starting with the \tc{assume} keyword declare that the
corresponding Haskell function honors the signature, but it isn't
checked. In this case, it's because \tc{Data.Set} is an external dependency that
Liquid Haskell can't check. But it can also be applied to our own functions.

Here \tc{Set\_mem} is a symbol that Liquid Haskell maps to the theory
of sets in the SMT solver. While Liquid Haskell doesn't check that
\tc{member} behaves as declared in the refinement type signature,
it will assume the property in the return refinement type
whenever \tc{member} is used in a program.

Notice how the predicate on the return type mentions both arguments. Liquid
Haskell lets us express refinement types which relate arguments with each other,
and with the result in this manner. This obviates the need, in particular, to
give a type-level name to arguments using existential quantification.

To define purely logic function, Liquid Haskell uses the \tc{measure} keyword,
such as:\improvement{Unfortunately, I couldn't think of a measure from the
  Liquid Haskell standard library that we use. So I resorted to take a random
  one from Set instead (which is also the only one from Set).}
\begin{verbatim}
measure listElts :: [a] -> Set a
  listElts []     = {v | (Set_emp v)}
  listElts (x:xs) = {v | v = Set_cup (Set_sng x) (listElts xs) }
\end{verbatim}

It is also possible to define uninterpreted symbols by simply omitting the
definition. It would look like this
\begin{verbatim}
measure listElts :: [a] -> Set a
\end{verbatim}
The meaning of the function would then be given by \tc{assume} refinement
type signatures on other functions.


\subsection{The rapier, refined}
\label{the-rapier-with-refinement-types}

We would, now, like to argue that using Liquid Haskell to enforce the
requirements from Section~\ref{the-rapier-with-stronger-types} is more
straightforward than using the type checker alone. The code presented in this section is available in the file
\sourcefile{Subst1.hs}.

In order to deal with scope checks, we first define a type alias \tc{ScopeExp S},
that is the type of all
expressions whose free variables are in the set \tc{S}\footnote{In type aliases,
Liquid Haskell expects parameter names to start with an uppercase letter.}.

\begin{verbatim}
{-@ type ScopedExp S = {e:Exp | isSubsetOf (freeVars e) S} @-}
\end{verbatim}

Functions like \tc{isSubsetOf} and \tc{difference} come from the \tc{Data.\allowbreak Set}
module. The function \tc{freeVars} is in the same module as \tc{subs\-ti\-tute},
and collects the free variables of an expression. We note that this function
is only used in refinement type signatures, and in particular, it is not evaluated
when calling to \tc{substitute}.

\begin{verbatim}
freeVars :: Exp -> Set Int
freeVars e = case e of
  Var i -> singleton i
  App e1 e2 -> union (freeVars e1) (freeVars e2)
  Lam i e -> difference (freeVars e) (singleton i)
\end{verbatim}

Next, we need to give the following refined signature to the \tc{freshVar} of
Figure~\ref{rapier-style-substitution}:
\begin{verbatim}
{-@ assume freshVar :: s:Set Int -> {v:Int | not (member v s)} @-}
\end{verbatim}
This signature is assumed rather than checked. We could choose to check it, but
Liquid Haskell doesn't have a good built-in understanding of the \tc{lookupMax}
function that we use. So instead, we choose to assume the signature. This is our
first principle of programming with refinement types:

\begin{principle}
  In general, refinement types allow to reduce the trusted code base, but they also offer
    you a choice. When its easier to prove a result by
    hand than with the SMT solver, you should assume the property.\improvement{I feel
    this principle needs a discussion of unsafe coercions. Are unsafe coercions the
    way to assume properties with abstract types? Then they also offer a choice as
    Liquid Haskell does. If unsafe coercions aren't such a good choice, why not?}
\end{principle}

That's it, this is the entirety of our trusted code base for this example. For
the most part, it required thinking about what properties we wanted to enforce,
but not much about how they ought to be enforced.\change{Aren't \tc{lookupSubst}
and \tc{extendSubst} part of our trusted code base too? If they aren't, I think
it would need to be explained. If they are, this account of the trusted code base
should be moved further below.}

Finally, we assume that we have some datatype representing substitutions
(\emph{i.e.} finite maps of variables to terms), although we don't require any
particular implementation: this datatype is a parameter of our algorithm. To
that effect we assume an opaque data type and axiomatize its supporting
functions.

\begin{verbatim}
data Subst t -- opaque
{-@ measure domain :: Subst e -> Set Int @-}

assume lookupSubst
  :: s:Subst Exp
  -> {k:Int | member k (domain s)}
  -> Exp

assume extendSubst
  :: s:Subst a
  -> i:Int
  -> a
  -> {v:Subst a | union (domain s) (singleton i) = domain v }
\end{verbatim}
\unsureF{Note that lookupSubst has a different type in the implementation now.
\tc{f :: forall <p :: Exp -> Bool>. Subst Exp<p> -> {k:Int | member k (domain s)} -> Exp<p>}
The abstract predicate \tc{p} is used to propagate properties as if lookupSubst were
polymorphic. I think we can gloss over this. The change was needed to make \tc{lookupSubst}
consistent with \tc{fromListSubst} whose domain contains all possible variables at the moment.
}

Notice that the logical function \tc{domain}, which stands for the set of
variables that the substitution defines, is uninterpreted. It must be since it's an
assumption.\unsure{Something which could be part of the discussion
  section at the end of the article, I think, is how do you actually instantiate
  these axioms with an concrete substitution type? Answer: Sounds good, you copy
  the assumptions to address whatever substitution representation is needed.}

We can give now the following signature to \tc{substitute}
\begin{verbatim}
{-@
substitute
  :: scope:Set Int
  -> s:Subst (ScopedExp scope)
  -> ScopedExp (domain s)
  -> ScopedExp scope
@-}
substitute :: Set Int -> Subst Exp -> Exp -> Exp
\end{verbatim}
Remarkably, the implementation for \tc{substitute} is unchanged from the
implementation, without static scope checking, of
Figure~\ref{rapier-style-substitution}. This won't always be the case, but this
exemplifies how using Liquid Haskell to enforce invariants tends to create less
boilerplate than a type-based approach.

Figure~\ref{rapier-style-substitution} uses that a substitution
\begin{verbatim}
s :: Subst (ScopedExp scope)
\end{verbatim}
also has (refined) type
\begin{verbatim}
s :: Subst (ScopedExp (insert i scope))
\end{verbatim}
This kind of subtyping is trivial with refinement types, it's the default
behavior. Whereas with an ML type system, subtyping isn't a typical feature. The
foil, for instance, needs an explicit function to cast substitutions when
extending a scope. This is our next principle:
\begin{principle}
  Refinement types add a layer of subtyping on top of your type system. When
  your program is best modeled with subtyping you should consider refinement
  types.
\end{principle}

The type of lambda terms is also unchanged, as the well-scoping invariant is
applied to a whole term at once. A nice consequence of it is that functions
that don't benefit from all the scope checking business can simply take a naked
term and ignore it. The \tc{freeVars} function, for example, is implemented on
naked terms.\unsure{I'm considering adding a comment on the fact that we can use
  freeVars to provide an initial scope for the substitute function. Note:
  the foil starts with an identity substitution, that is, one that maps every
  variable to itself. And we do the same in our unification code. The substitution
  domain can be narrowed if desired, but starting with the identity doesn't affect
  the static checks.}

\subsection{A hybrid approach}
\label{ensuring-the-scope-set-is-checked}

Our refinement type signature of \tc{substitute} follows the type signature of
\citeauthor{maclaurin23} to the letter.
Yet we can introduce the following bug in \tc{substitute} from
Figure~\ref{rapier-style-substitution}:
\begin{verbatim}
  ...
  | member i scope ->
      Lam i $ substitute (insert i scope) (extendSubst s i (Var i)) e
\end{verbatim}
Liquid Haskell flags no errors but the program will still misbehave as
follows (in pseudo-Haskell).

$$\codeblocksize{\tc{substitute}~\{\tc{x}\}~(\lambda{}\tc{x}. \tc{y}) [\tc{y}:=\tc{x}] = (\lambda{}\tc{x}. \tc{x})}$$

What is going on? The binder \tc{i} is now capturing free variables in the
range of the substitution. The signature is, in fact, indifferent to whether
the binder \tc{i} is already present or not in the scope set. There's no
mechanism to prevent adding a binder that is already present in the scope set.
That is, we fail to enforce Property~(\ref{req:always-rename}) from Section~\ref{the-rapier-with-stronger-types}.
And, more to the point, how could we? ‚ÄúNever add a binder to the scope set that is already
present‚Äù isn't a set theoretical property. It's not even a functional property.
It is a kind of temporal invariant.

Such temporal invariants aren't naturally expressed in the logic of Liquid Haskell.
But they're quite easy to implement with abstract types. So let's use an abstract
type. What we need to do is to ensure that whenever we see a new binder it must
be tested against the scope, and that this test is packaged together with fresh
name generation.

We follow the foil and
introduce an abstract type \tc{Scope} and a function \tc{withRefreshed}. The types are a little
simpler because we don't need existential quantification to reflect value-level
objects at the type level, but otherwise these are the same functions and types
as in Section~\ref{the-rapier-with-stronger-types}.
\begin{verbatim}
newtype Scope = UnsafeScope { unsafeUnScope :: (Set Int) }
{-@
predicate Member E S = Set.member E (unsafeUnScope S)

withRefreshed :: s:Scope -> i:Int
  -> {p:(Scope, Int) |
       not (Member (snd p) s) && fst p == union s (singleton (snd p))}
@-}
withRefreshed :: Scope -> Int -> (Scope, Int)
withRefreshed (UnsafeScope s) i
  | Set.member i s = let j = freshVar s in (UnsafeScope (insert j s), j)
  | otherwise = (UnsafeScope (insert i s), i)
\end{verbatim}

We needed to add a refinement type signature to \tc{withRefreshed} to serve as
glue with the Liquid Haskell world. This refinement type signature tells Liquid
Haskell precisely that \tc{withRefreshed} does both membership checking and
fresh variable call: the variable returned by \tc{withRefreshed} isn't in the
old scope but is in the new scope.

It then suffices to seal the type \tc{Scope} to enforce that binders are always
refreshed when traversed. The users are discouraged from manipulating directly
the scope representation that we chose, so \tc{withRefreshed} remains the only
way to test for membership and to extend a scope. This is why we define a
\tc{Member} predicate alias, only available in the logic, but provide no
\tc{member} function in Haskell for \tc{Scope}s.
The full code for this example can be found in the
file \sourcefile{Subst2.hs}.

This is our next principle for refinement types:
\begin{principle}
  Refinement types and abstract types are best at enforcing different kind of
  properties. You should user the simpler solution for each property that you
  need, as refinement types and abstract types mix well.
\end{principle}

% The second approach, avoid introducing \tc{withRefreshed} and modifying the
% types in the program by providing a more stringent refinement type for
% \tc{substitute}.

% \begin{verbatim}
% substitute
%   :: scope:Set Int
%   -> s:Subst (ScopedExp scope)
%   -> ei:ScopedExp (domain s)
%   -> {v:Exp | freeVars v == freeVarsSubst (freeVars ei) s}
% \end{verbatim}

% In this signature we are spelling exactly what the expected free variables in the result are.
% We use a function \tc{freeVarsSubst} such that
% \tc{freeVarsSubst (freeVars e) s} computes the free variables in the range of the
% substitution \tc{s} that is actually used when applying it
% to the expression \tc{e}. We provide an example representation for substitutions and
% a schematic presentation of \tc{freeVarsSubst}, but in our implementation we
% are careful to keep the checks agnostic on the actual representation of
% substitutions.

% \begin{verbatim}
% type Subst e = [(Int, e)]
% freeVarsSubst :: Set Int -> Subst Exp -> Set Int
% freeVarsSubst used [] = empty
% freeVarsSubst used ((i, e) : xs) =
%   | member i used = -- only take the first occurrence of i
%       union (freeVars e) (freeVarsSubst (difference used (singleton i)) xs)
%   | otherwise = freeVarsSubst used xs
% \end{verbatim}

% Unless we rename all the binders unconditionally, it is no longer
% possible to ignore the scope set when going under binders since the calculation
% of free variables doesn't add up:
% If the name of the binder is in the free variables of the range of the substitution,
% it may show up in the free variables of the result, but leaving it unrenamed would
% cause the call to the function \tc{freeVars} in the expected return refinement type
% to disagree.

% Unfortunately, this refinement type signature is more laborious to check, as
% \tc{free\-Vars\-Subst} requires roughly one lemma per case of the \tc{subs\-ti\-tute}
% function.

% \begin{verbatim}
% lemmaFreeVarsSubstSing :: i:_ -> s:_
%   -> { freeVarsSubst (singleton i) s == fromMaybe empty (lookupSubst i s) }

% lemmaFreeVarsSubstUnion :: s1:_ -> s2:_ -> s:_
%   -> { freeVarsSubst (union s1 s2) s
%          == union (freeVarsSubst s1 s) (freeVarsSubst s2 s) }

% lemmaFreeVarsSubstExtend
%   :: scope:_ -> used:_
%   -> i:_ -> {e:_ | Data.Set.null (intersection (freeVars e) scope)}
%   -> s:Subst (ScopedExp scope)
%   -> { freeVarsSubst (difference used (singleton i)) s ==
%        difference (freeVarsSubst used (extendSubst s i e)) (freeVars e)
%      }
% \end{verbatim}

% Each lemma requires writing a recursive function for Liquid Haskell to
% check it, which is additional effort. Here's the proof for
% \tc{lemma\_free\-Vars\-Subst\_union}.

% \begin{verbatim}
% lemmaFreeVarsSubstUnion :: Set Int -> Set Int -> Subst Exp -> ()
% lemmaFreeVarsSubstUnion _ _ [] = ()
% lemmaFreeVarsSubstUnion s1 s2 ((i, _) : xs) =
%   lemmaFreeVarsSubstUnion
%     (difference s1 (singleton i)) (difference s2 (singleton i)) xs
% \end{verbatim}

% The recursive function follows the structure of an inductive proof,
% with much of the folding, unfolding, and set properties applied
% automatically. And this is similar for the proofs of the other lemmas.
% Then the lemmas need to be applied in the cases of \tc{substitute}.

% \begin{verbatim}
% substitute scope s e0 = case e0 of
%   Var i -> case lookupSubst i s of
%     Nothing -> e0
%     Just e -> e ? lemmaFreeVarsSubstSing i s
%   App e0 e1 ->
%     App (substitute scope s e0) (substitute scope s e1)
%       ? lemmaFreeVarsSubstUnion (freeVars e0) (freeVars e1) s
%   Lam i e
%     | member i scope,
%       let j = freshVar scope ->
%         Lam j $ substitute (insert j scope) (extendSubst s i (Var j)) e
%           ? lemmaFreeVarsSubstExtend scope (freeVars e) i (Var j) s
%     | otherwise ->
%         Lam i $ substitute (insert i scope) (extendSubst s i (Var i)) e
%           ? lemmaFreeVarsSubstExtend scope (freeVars e) i (Var i) s
% \end{verbatim}

% The operator \tc{?} is an alias for function \tc{const = \textbackslash x \_ -> x} and
% brings the lemma into consideration of Liquid Haskell when checking the
% first argument without evaluating the recursive function that stands for
% the lemma proof. This solution is available in the file
% \tc{Subst3.hs}\footnote{A solution that addresses all requirements without
%   changing types in the program: \sourcefile{Subst3.hs}}\change{A single Subst
%   file please}
% in our repository.




\section{Unification}
\label{unification}

% hereditary Harrop formulas~\cite{miller91}
Now that we have established the refined rapier interface, let us show how it
can be applied to a more realistic example: solving first-order equational
formulas. Specifically, we'll be solving a form of Horn clauses in
the Herbrand domain. This is the sort of unification problem which can show up
when typechecking programs with GADTs~\cite{schrijvers09}. Scope management in
such a solver is a much trickier business than in the case of mere substitutions
and, in the authors' experience, something where any help from the compiler is welcome.
The source code of this section can be found in the file
\sourcefile{Unif.hs}.

In addition to variables, still represented as integers, we have unification
variables. Unification variables have their own scopes: the formula
$\exists x. \forall y. x=y$ doesn't have a solution. It will be reduced to a
formula of the form $f_{x} = y$ where $f_{x}$ is a unification variable; we very much
don't want this unification problem to succeed\improvement{I think we should
  use a better notation to distinguish unification variables from rigid
  variables.}: we shall make it so that $y$ isn't in the permissible scope for $f_{x}$.

Furthermore, the unification algorithm will perform substitutions. Substitutions are blocked by
unification variables as we don't know what they stand for yet. So a unification
variable, in our syntax, is a pair $(f, [x_0:=t_0,\ldots,x_n:=t_n])$ of a
unification variable proper and a suspended substitution. Where
$\{x_0,\ldots,x_{n}\}$ is the scope of $f$. Such a pair is akin to a skolem
function application $f(t_0,\ldots,t_n)$. Notice in particular, how the solution
of $f$ can only have free variables in $\{x_0,\ldots,x_{n}\}$, but
$(f, [x_0:=t_0,\ldots,x_n:=t_n])$ may live in a different scope altogether.
That there is multiple intermingled scopes to manage, rather than one like
in the case of substitution (Section~\ref{capture-avoiding-substitution})
is what makes this type of unification problem tricky.

\begin{verbatim}
type Var = Int
type SkolemApp = (Var, Subst Term)
\end{verbatim}

This way, our formula $\exists x. \forall y. x=y$ will be reduced to
$(f_{x},[]) = y$ which doesn't have a solution. On the other hand
$\forall x. \exists y. x = y$ becomes $x = (f_{y}, [x:=x])$ so $x$ is a solution
for $f_{y}$ and the formula is solvable.

Our unification algorithm is a first-order variant of pattern
unification~\cite{miller91-pattern} sufficient to eliminate equalities to the
left of implication in the style proposed by~\citet{miller22}. The main
functions, sans refined signatures, can be found in
Figure~\ref{conditional-unification}. Unification algorithms can get pretty
finicky, for the sake of simplicity our algorithm isn't as complete as it could
be and will miss some solutions\footnote{We have, on the other hand, tried to
  make the algorithm correct, so if it finds unsound solution it's a bug and we
  apologize.}.

At the heart of the algorithm is substitution inversion~\cite{ziliani15}: when
encountering an equality of the form
$$(f_{x},[y:=a, z:=b]) = u$$
If there is a solution, we want it to be
$$f_{x} := u[a:=y, b:=z]$$
This is the same as pattern unification, except that it doesn't need terms to
contain functions. The \tc{inverseSubst} function is responsible for this
inversion.

We're choosing a language of term with both regular variable (representing
variables bound by universal quantifiers), skolem applications representing
unification variables with their substitutions, as well as sufficient
constructors to encode arbitrary terms. Here is the concrete type of term, as
well as that of formula where the only thing to remark is that the left-hand
side of implications is a single equality.

\begin{verbatim}
data Term
  = V Var | SA SkolemApp | U | L Term | P Term Term deriving

data Formula
  = Eq Term Term               -- equality
  | Conj Formula Formula       -- conjunctions
  | Then (Term, Term) Formula  -- a = b => f
  | Exists Var Formula         -- existential quantification
  | Forall Var Formula         -- universal quantification
\end{verbatim}

In Figure~\ref{conditional-unification}, the function \tc{unify} takes a rapier scope parameter containing all the variables that
can appear free in the input formula. This set is used to rename \tc{Forall}
binders when doing substitutions. For instance, unifying the following formula
$$\forall x. \forall y. \exists z. y = L(x) \Rightarrow \forall x. y = z$$
reduces to unifying
$$\forall x. \forall y. \exists z. (\forall x. y = z)[y:=L(x)]$$
and the substitution needs to rename the inner binder $x$.

In a preceding pass (Section~\ref{sec:skolemize}), existential quantifiers are replaced
with skolem applications, so in \tc{unify} we assume that there is no
existential quantifier. We have functions \tc{substituteFormula} and \tc{substitute}
to apply substitutions in formulas and terms respectively, and \tc{substituteSkolems} to  substitute
unification variables in formulas. We have a function \tc{skolemSet} to collect the skolem applications of a
term. And a function \tc{fromListSubst} to construct a substitution from a list of
pairs \tc{[(Var, Term)]}.

The functions \tc{substEq} and \tc{unifyEq} are simplified here to their barest
essence. In particular, \tc{unifyEq} never fails in this simple form. They handle
more cases in the accompanying artifact, but these cases aren't essential to our
discussion.

\begin{figure}
\begin{verbatim}
unify :: Set Int -> Formula -> Maybe [(Var, Term)]
unify s (Forall v f) = unify (Set.insert v s) f
unify s (Exists v f) = error "unify: the formula hasn't been skolemized"
unify s (Conj f1 f2) = do
    unifyF1 <- unify s f1
    unifyF2 <- unify s (substituteSkolems f2 unifyF1)
    return (unifyF1 ++ unifyF2)
unify s f@(Then (t0, t1) f2) =
    let subst = fromListSubst (substEq t0 t1)
     in unify s (substituteFormula s subst f2)
unify s (Eq t0 t1) = unifyEq t0 t1

substEq :: Term -> Term -> [(Var, Term)]
substEq (V i) t1 = [(i, t1)]
substEq t0 (V i) = [(i, t0)]
substEq _ _ = []

unifyEq :: Term -> Term -> [(Var, Term)]
unifyEq t0 t1@(SA (i, s))
  | Just s' <- inverseSubst $ narrowForInvertibility (freeVars t0) s
  , let t' = substitute s' t0
  , not (Set.member i (skolemSet t'))
  , Set.isSubsetOf (freeVars t') (domain s)
  = [(i, t')]
unifyEq t0@(SA _) t1 = unifyEq t1 t0
unifyEq _ _ = []

-- | @narrowForInvertibility vs s@ removes pairs from @s@ if the
-- range is not a variable, or if the range is not a member of @vs@.
narrowForInvertibility :: Set Var -> Subst Term -> Subst Term
narrowForInvertibility vs (Subst xs) =
  Subst [(i, V j) | (i, V j) <- xs, Set.member j vs]

inverseSubst :: Subst Term -> Maybe (Subst Term)
inverseSubst (Subst xs) = fmap Subst (go xs)
  where
    go [] = Just []
    go ((i, V j) : xs) = fmap ((j, V i) :) (go xs)
    go _ = Nothing
\end{verbatim}
\caption{Conditional unification}
\Description{Haskell implementation of condition unification}
\label{conditional-unification}
\end{figure}

The function \tc{unifyEq} defines what a good solution should be.
One of the conditions is that whatever term \tc{t'} is proposed
as solution for a skolem \tc{i}, it needs to have as free variables only those in the
codomain of the substitution defining the skolem application
(\textit{scope check}). Another
condition is that the skolem \tc{i} should not occur in the solution
\tc{t'} (\textit{occurs check}). And since we are inverting a substitution to find
\tc{t'}, we might not find solutions if we cannot invert the
substitution. This implementation only inverts substitutions where
variables are mapped to variables. That is, we solve $(f, [z:=x]) = L(L(x))$
to get the solution $(f, L(L(z)))$ but we do not try solving $(f, [z:=L(x)]) = L(L(x)))$.


\subsection{A look at skolemization}
\label{sec:skolemize}

Figure~\ref{skolemization} shows the function to replace existential quantifiers
with unification variables. This example is interesting because the complexity of
managing two different scopes for universal and existential quantifiers
respectively exceeds considerably the canonical example of the rapier.

\begin{figure}
\begin{verbatim}
skolemize :: Set Int -> Formula -> State (Set Int) Formula
skolemize sf (Forall v f) = do
    se <- get
    put (Set.insert v se)
    f' <- skolemize (Set.insert v sf) f
    pure (Forall v f')
skolemize sf (Exists v f) = do
    se <- get
    let u = if Set.member v se then freshVar se else v
    put (Set.insert u se)
    let subst = fromListSubst [(v, SA (u, fromSetIdSubst sf))]
    skolemize sf (substituteFormula sf subst f)
skolemize sf (Conj f1 f2) = do
     f1' <- skolemize sf f1
     f2' <- skolemize sf f2
     pure (Conj f1' f2')
skolemize sf (Then (t0, t1) f2) = do
     f2' <- skolemize sf f2
     pure (Then (t0, t1) f2')
skolemize _ f@Eq{} = pure f
\end{verbatim}
\caption{Skolemization}
\Description{Haskell implementation of skolemization}
\label{skolemization}
\end{figure}

To begin with, the function takes a set \tc{sf} of variables that have been
introduced with universal quantification, and that can appear free in the
input formula. The set \tc{se} in the monadic state contains the variables
that have been introduced with existential quantification \tc{se}.

We expect the set \tc{sf} to be a subset of \tc{se}. This is to reflect the
fact that, for debugging purposes, we don't want unification variables to be called the same as
universally quantified variables. It is not a strict requirement, but one
that makes the output of \tc{skolemize} considerably easier to read.

We also really need to put the set \tc{se} in the monadic state, as we don't
want to generate the same unification variable for existentials appearing
on different subformulas, since unification variables scope over the entire formula. For instance, the following formula
$$\forall x. \exists y. x = y \land \forall z. \exists y. z = y$$
should produce unification variables like
$$\forall x. x = y[x:=x] \land \forall z. z = w[x:=x, z:=z]$$
It would be a mistake to call both unification variables $y$ and $w$ the same.
Their occurrences even have different scopes!

Lastly, we do need to keep the scope set \tc{sf} separate from the monadic
state because it is needed to construct the skolem function applications where
existential variables are found.

Here is the refinement type signature of \tc{skolemize}.
\begin{verbatim}
skolemize
  :: sf:_
  -> {f:ScopedFormula sf | consistentSkolemScopes f}
  -> State
       < {\se0 ->
             isSubsetOf sf se0
          && isSubsetOf (IntMapSetInt_keys (scopes f)) se0
         }

       , {\se0 v se ->
             consistentSkolemScopes v
          && existsCount v = 0
          && isSubsetOf (freeVarsFormula v) sf
          && isSubsetOf se0 se
          && Set.empty =
               intersection
                 se0
                 (IntMapSetInt_keys
                    (IntMap.difference (scopes v) (scopes f)))
          && intMapIsSubsetOf (scopes f) (scopes v)
          && isSubsetOf (IntMapSetInt_keys (scopes v)) se
       }>
       _ _
     / [formulaSize f]
\end{verbatim}

This type signature is, admittedly, a bit involved. However while we were
designing this case study, \tc{skolemize} stayed without a refined signature
until pretty much the very end. This is possible because the inherent subtyping
of refinement types makes it easy to use unrefined and refined functions together.
Of course this prevented us from having guarantees for the program end-to-end,
but it is fine to add guarantees only where you need them. What you choose
to harden will not have to infect the rest of the program.
Which leads us to our next principle

\begin{principle}
  Functions with refined signature and without mix well. You should first use
  refinement types on function with the best power-to-weight ratio. You can
  incrementally add stronger types on more functions as your program evolves.
\end{principle}

Liquid Haskell helpfully lets us treat the state monad as equipped with
Hoare triples \tc{State<pre,post>}. The supporting code for the refined state
monad is not readily available in Liquid Haskell. It probably should be, but in
the meantime, it can be found in Liquid Haskell's test suite, so we simply
copied it in the file \sourcefile{State.hs}.

The main conjuncts of the postconditon are \tc{consistent\-Skolem\-Scopes v} and
\tc{existsCount v = 0}, the rest are invariants used by the recursive calls of
\tc{skolemize}.
\begin{itemize}
  \item \tc{existsCount v = 0} means that \tc{skolemize} returns a formula without
  existential quantifiers. As it is a requirement of \tc{unify}.
  \item \tc{consistentSkolemScopes v} means that \tc{skolemize} returns a
  formula $F$ such that all the occurrences of any unification variable $i$ in $F$
  have the same scope. This is our main scope invariant for this section.
\end{itemize}

Incidentally, we found that Liquid Haskell hangs when checking the \tc{Exists}
case of \tc{skolemize}. By resorting to the principle of only checking what is
cost-effective, we disabled checking for that specific case, while we still verify
the rest of the function.
\unsure{Arnaud removed the paragraph on stuff hanging. Maybe we want to discuss this
  limitation in the conclusion. Facundo added a rephrasing of it again here.}

% We won't discuss the checking process for this function, as examples follow
% which  don't require the burden of explaining the mechanisms for
% checking monadic code. But we do note that all the cases of the function
% \tc{skolemize} are checked as it appears in Figure~\ref{skolemization},
% except for the case of existential quantifiers which cause Liquid Haskell
% to hang at the moment.\unsureF{Such a terrible ending for a great section. It is the SMT solver that seems to be hanging on some query.}


\subsection{The theory of \tc{unifyEq}}
\label{checking-unifyEq}

Let us now turn to the \tc{unifyEq} function, which is a traditional unification
function: it takes an equation and returns a list of unifications.
It is simplified to
never fail for conciseness, but it can indeed fail in the artifact.

We start introducing static checks by providing a refinement type signature to
the function \tc{unifyEq}, which encodes the scope check, the occurs check,
and the relationship of skolems present in the result and in the arguments.

\begin{verbatim}
unifyEq
  :: {t0:Term | consistentSkolemScopesTerm t0}
  -> {t1:Term |
          unionCommutes (scopesTerm t0) (scopesTerm t1)
       && consistentSkolemScopesTerm t1
     }
  -> [(v :: Var
      , { t:Term |
            isSubsetOfJust (freeVars t)
              (IntMap.lookup v
                (IntMap.union (scopesTerm t0) (scopesTerm t1)))
          && not (Set.member v (skolemSet t))
          && intMapIsSubsetOf (scopesTerm t)
               (IntMap.union (scopesTerm t0) (scopesTerm t1))
          && consistentSkolemScopesTerm t
        }
      )]
\end{verbatim}

The function \tc{scopesTerm} is only used in refinement types and it produces
a projection of the skolem applications in a given term. Instead of returning
full skolem applications, it only provides the skolem name and the domain of
the substitution.

\begin{verbatim}
scopesTerm :: Term -> IntMap (Set Int)
scopesTerm (V i) = IntMap.empty
scopesTerm (SA (i, s)) = IntMap.insert i (domain s) (scopesSubst s)
scopesTerm U = IntMap.empty
scopesTerm (L t) = scopesTerm t
scopesTerm (P t0 t1) = IntMap.union (scopesTerm t0) (scopesTerm t1)
\end{verbatim}

We would like to draw the reader's attention to the precondition
\begin{verbatim}
unionCommutes (scopesTerm t0) (scopesTerm t1)
\end{verbatim}
The \tc{unionCommutes m1 m2} predicate states that \tc{union m1 m2 = union m2
  m1}, which in turn, because \tc{union} is the left-biased union of maps,
means that any key defined in both \tc{m1} and \tc{m2} have the same associated
value in both. This is the binary version of our
\tc{consistent\-SkolemScopesTerm} predicate. The statement is a bit indirect,
but that's for a good reason. Since our version of Liquid Haskell understands
the theory of finite maps over integers
(Section~\ref{extending-liquid-haskell}), this statement is going to
be translated to a theory of the SMT solver rather than a complex formula. Which
constitutes our next principle.

\begin{principle}
  SMT solvers are more effective when they can leverage their theories.
  Prefer writing refinement types by appealing to theories known by your system.
\end{principle}

In fact this same expression is used in the definition of
\tc{consistent\-SkolemScopesTerm} for the very same reason.

\begin{verbatim}
consistentSkolemScopesTerm :: Term -> Bool
consistentSkolemScopesTerm (V _) = True
consistentSkolemScopesTerm SA{} = True
consistentSkolemScopesTerm U = True
consistentSkolemScopesTerm (L t) = consistentSkolemScopesTerm t
consistentSkolemScopesTerm (P t0 t1) =
    consistentSkolemScopesTerm t0 && consistentSkolemScopesTerm t1
    && unionCommutes (scopesTerm t0) (scopesTerm t1)
\end{verbatim}

As a side note, \tc{unifyEq} is an example of a function whose code needs to be
annotated to help the Liquid Haskell check the refined signature. We've needed
a handful of lemmas. Here's one of them. Its proof is a regular (and rather
straightforward) recursive function

\begin{verbatim}
{-@
lemmaSubstituteScopesTerm
  :: s:Subst {ti:Term | isVar ti}
  -> t:Term
  -> { scopesTerm t = scopesTerm (substitute s t) }
 @-}
lemmaSubstituteScopesTerm :: Subst Term -> Term -> ()
lemmaSubstituteScopesTerm s (SA (_, s1)) = lemmaComposeSubstDomain s1 s
lemmaSubstituteScopesTerm s U = ()
lemmaSubstituteScopesTerm s (L t) = lemmaSubstituteScopesTerm s t
lemmaSubstituteScopesTerm s (P t0 t1) =
    lemmaSubstituteScopesTerm s t0 `seq` lemmaSubstituteScopesTerm s t1
lemmaSubstituteScopesTerm s (V v) = case lookupSubst s v of
    V _ -> ()
    _ -> ()
\end{verbatim}

The proof uses itself another lemma, which establishes a property on substitutions,
and therefore it is assumed as another expectation we have of their implementation.

\begin{verbatim}
assume lemmaComposeSubstDomain
  :: s0:Subst Term -> s1:Subst {t:_ | isVar t}
  -> {   domain s0 == domain (composeSubst s0 s1)
      && scopesSubst s0 == scopesSubst (composeSubst s0 s1) }
\end{verbatim}

Lemmas are ordinary functions, Liquid Haskell doesn't automatically make use of
them. But Liquid Haskell comes with an idiom. When you want to ask Liquid
Haskell to use a lemma when checking the expected refinement type of an expression
\tc{e}, you are to invoke the lemma as
\begin{verbatim}
e ? lemmaSubstituteScopesTerm
\end{verbatim}
The changes to \tc{unifyEq} are annotations of this form, where \tc{?} has no
effect when executing the program.


\subsection{Totality and \tc{unify}}
\label{checking-unify}

There is not much more to add for the \tc{unify} function, but let us take this
opportunity to talk about the totality requirement. Here is its signature.

\begin{verbatim}
unify
  :: s:Set Int
  -> {f:Formula | consistentSkolemScopes f && existsCount = 0}
  -> [(v :: Var
      , { t:Term |
             isSubsetOfJust (freeVars t) (IntMap.lookup v (scopes f))
          && not (Set.member v (skolemSet t))
          && IntMap.isSubsetOf (scopesTerm t) (scopes f)
          && consistentSkolemScopes f
          && existsCount f = 0
        }
      )] / [formulaSize f]
\end{verbatim}

Notice the precondition \tc{existsCount = 0}. It is not optional. Indeed, the
\tc{Exists} case of \tc{unify} in Figure~\ref{conditional-unification} raises an
error. Liquid Haskell, however, requires functions to be total. We need this
precondition so that Liquid Haskell can prove that this case never occurs.

This totality requirement isn't necessary to refinement types in general.
However, in the case of Haskell, laziness lets us write
\begin{verbatim}
{-@ bad :: () -> { false } @-}
bad :: () -> ()
bad _ = let {-@ f :: { false } @-}
            f = error "never happens"
         in (\_ -> ()) f
\end{verbatim}
It may seem that Liquid Haskell could accept this function because \tc{f} appears
to prove \tc{false}. In a strict language this wouldn't be a big problem as
\tc{bad} would loop and any attempt at using \tc{bad} would diverge. But \tc{bad} is
actually a total function. Liquid Haskell rejects \tc{bad} because it fails to
prove that \tc{f} is total, hence refuses to accept its signature.

This is also why the signature of \tc{unify} ends with \tc{/ [formulaSize f]}.
Liquid Haskell needs to prove that \tc{unify} terminates and, because of the
substitutions, \tc{unify} isn't a structurally recursive function. So Liquid
Haskell needs a little help in the form of a termination metric. We use here
the number of connectives in the argument formula, which is unaffected by
substitution since we only substitute inside terms.

% The recursion of unify is not structural since in the conjunction and implications
% cases we are not feeding subformulas. Therefore we provide a termination
% metric \tc{formulaSize} that counts the amount of connectives in a formula.
% We have a function \tc{existsCount} that yields the amount of existential
% quantifiers in a formula.
% We also define a function \tc{scopes} analogous to \tc{scopesTerm} to obtain the skolems
% of a formula.

% \begin{verbatim}
% scopes :: Formula -> IntMap Var (Set Int)
% scopes (Forall _ f) = scopes f
% scopes (Exists _ f) = scopes f
% scopes (Conj f1 f2) = IntMap.union (scopes f1) (scopes f2)
% scopes (Then (t0, t1) f2) =
%   IntMap.union (scopesTerm t0) (IntMap.union (scopesTerm t1) (scopes f2))
% scopes (Eq t0 t1) = IntMap.union (scopesTerm t0) (scopesTerm t1)
% \end{verbatim}

% We also define the predicate \tc{consistentSkolemScopes}, analogous to
% \tc{consistentSkolemScopesTerm} which ensures that skolem scopes are
% consistent in a formula.

% Liquid Haskell can check mostly automatically the refinement type signature of
% \tc{unify}. In the equations for quantifiers, unfolding definitions of
% \tc{scopes} plus the refinement type of the recursive calls suffices.
% In the equation of conjunction, the theory of arrays of the SMT solver
% kicks in to establish the first conjunct of the refinement type:

% \begin{verbatim}
% unionCommutes (scopes f1) (scopes f2)
% =>
% (isSubsetOfJust (freeVars t) (IntMap.lookup v (scopes f1)) =>
%   isSubsetOfJust
%     (freeVars t)
%     (IntMap.lookup v (IntMap.union (scopes f1) (scopes f2))))
% &&
% (isSubsetOfJust (freeVars t) (IntMap.lookup v (scopes f2)) =>
%   isSubsetOfJust
%     (freeVars t)
%     (IntMap.lookup v (IntMap.union (scopes f1) (scopes f2))))
% \end{verbatim}

% The antecedent of the above goal is established by \tc{consistent\-Skolem\-Scopes}
% in the refinement type of the input. The consequent relates the refinement types
% of the recursive calls to the expected refinement type of the result, as Liquid
% Haskell knows that \tc{IntMap.\allowbreak union (scopes f1) (scopes f2)} stands for
% \tc{scopes (Conj f1 f2)}. An additional lemma is also needed to convince Liquid
% Haskell that \tc{substituteSkolems} is going to preserve \tc{consistentSkolemScopes}.
% This lemma relates the scopes of a substitution with the scopes in any map.
% \begin{verbatim}
% lemmaScopesListSubset
%   :: m0:IntMap (Set Int)
%   -> s:[(Var, {t:Term | intMapIsSubsetOf (scopesTerm t) m0})]
%   -> { intMapIsSubsetOf (scopesList s) m0 }
% \end{verbatim}
% Here \tc{scopesList} is a function that collects the skolem scopes in a list of
% pairs, and the lemma is instantiated with the scopes of the input formula.
% \begin{verbatim}
% unify s f@(Conj f1 f2) = do
%     unifyF1 <- unify s f1
%     let lemmaSubst = lemmaScopesListSubset (scopes f) unifyF1
%     unifyF2 <- unify s (substituteSkolems (f2 ? lemmaSubst) unifyF1)
%     return (unifyF1 ++ unifyF2)
% \end{verbatim}
% This lemma helps convince the SMT solver that the union of \tc{scopes f2}
% and \tc{scopesList unifyF1} commutes, that is the scopes in the arguments of
% \tc{substituteSkolems}, which is a necessary condition for ensuring that
% \tc{consistentSkolemScopes} is preserved.

% The second conjunct of the return refinement type of \tc{unify}
% is accepted as it is established by the recursive calls and \tc{unifyEq}
% as is. And the other equations in \tc{unify} are checked using similar reasoning,
% all of it automatic except for a lemma similar to \tc{lemmaScopesListSubset} but
% for the implication equation.


\subsection{Extending Liquid Haskell to support \tc{IntMap}}
\label{extending-liquid-haskell}

Our unification case study uses the theory of finite maps. Liquid Haskell,
however doesn't support a theory of finite maps\footnote{Issue to support maps in
  the Liquid Haskell repository:
  \url{https://github.com/ucsd-progsys/liquidhaskell/issues/2534}}. It is
possible to do without it. In a first approximation we did much of this study in
vanilla Liquid Haskell. But we lost on automation: the code was more convoluted,
and we got a lot of lemmas to prove and pass around.

To support this study, we implemented the theory of finite maps for Liquid
Haskell. It's not in a mergeable state yet, for one thing: we only support finite
maps with \tc{Int} as their domain and \tc{Set Int} as their codomain. It could easily be adapted for any fixed
domain and codomain types, but it's not yet a general solution that can be instantiated at any
domain or codomain type. But our ultimate intent is to upstream these changes. Our
modifications can be found in the files \sourcefile{ifl25-liquidhaskell.patch} and \sourcefile{ifl25-liquid-fixpoint.patch}.

The theory of finite map is a good example of a theory that Liquid Haskell wants
to support: it is both powerful, and widely applicable. Pragmatically, it's also
one that is reasonably easy to support with SMT solvers by translating it to
the theory of arrays.

On the syntax front, Liquid Haskell allows to link a Haskell type with a particular
representation in the SMT solver.

\begin{verbatim}
{-@ embed IntMap * as IntMapSetInt_t @-}
\end{verbatim}

Here we are indicating that \tc{IntMap b} must be represented as \tc{IntMapSetInt\_t}
in the logic. \tc{IntMapSetInt\_t} is an alias for \tc{Array Int (Option (Set Int))}.
An array is an entity that associates keys with values, and which has an equality predicate,
and it is defined as one of the theories in SMT-LIB, the standard interface
to SMT solvers~\cite{BarFT-RR-25}.
The keys in this case are integers, and the values are either \tc{None} if the key
is not in the map, or \tc{Some s} if the key maps to a set \tc{s}. The
\tc{Option} type is a copy of Haskell's \tc{Maybe}.
We do not reuse \tc{Maybe} as Liquid Haskell's framework to connect to the SMT solver is
reused for other languages (\emph{e.g.} \cite{lehmann23}), and we prefer to keep
the implementation free of language specific details.
Here is the declaration of the \tc{Option} data type in SMT-LIB.

\begin{verbatim}
(declare-datatype Option (par (a) (None (Some (someVal a)))))
\end{verbatim}

We arranged for Liquid Haskell to include this declaration in the preamble of any
queries to the SMT solver. The types \tc{Array}, \tc{Int}, and \tc{Set} are already
known to the tooling.
It doesn't matter what type \tc{b} is instantiated to, the \tc{embed} annotation will
always set the same representation for \tc{IntMap b}, and this is a limitation that
would need to be addressed to support maps properly.

The array theory allows to describe how to retrieve the value associated with
a key, and how to update the value. On the Haskell front, we link these operations
to those of the \tc{IntMap b} type.

\begin{verbatim}
define IntMap.empty = (IntMapSetInt_default None)
define IntMap.insert x y m = IntMapSetInt_store m x (Some y)
define IntMap.lookup x m =
  if (isSome (IntMapSetInt_select m x)) then
    (GHC.Internal.Maybe.Just (someVal (IntMapSetInt_select m x)))
  else
    GHC.Internal.Maybe.Nothing
\end{verbatim}

The operations \tc{IntMapSetInt\_default}, \tc{IntMapSetInt\_store}, and \tc{IntMapSetInt\_select}
are aliases that we implemented in Liquid Haskell to call to the array operations.
In the case of \tc{lookup}, we translate the \tc{Option} type to Haskell's \tc{Maybe}.

The implementation of union, intersection,
difference, and subset checks for maps, however,
need operations beyond the standard interface, and not all SMT solvers can support
them. In our implementation we used the \tc{map} operation of the
Z3 SMT solver. The following snippet contains the implementation of
\tc{intMapIsSubsetOf} in SMT-LIB, and we also feed these declarations to the
SMT solver in a preamble to the queries.

\begin{verbatim}
; Similar to do {a0 <- oa0; a1 <- oa1; guard (a0 /= a1); pure a0}
(define-fun difference_strict_p2p
  ((oa0 (Option (Set Int)))
   (oa1 (Option (Set Int))))
  (Option (Set Int))
  (match oa0
    ((None None)
     ((Some a0) (match oa1
                  ((None oa0)
                   ((Some a1) (ite (= a0 a1) None oa0))))))))

; Similar to: empty == zipWith difference_strict_p2p xs ys
; where zipWith applies the function pointwise to the values in the
; arrays
(define-fun IntMapSetInt_isSubsetOf
  ((xs (Array Int (Option (Set Int))))
   (ys (Array Int (Option (Set Int)))))
  Bool
  (= ((as const (Array Int (Option (Set Int)))) None)
     ((_ map IntMapSetInt_difference_strict_p2p) xs ys)))
\end{verbatim}

Besides the limitation of the \tc{embed} annotation, another barrier for
proper support is that old versions of SMT-LIB require user defined
functions to have monomorphic types. This means, for instance, that
the type of \tc{IntMapSetInt\_isSubsetOf} cannot be generalized to work
on any \tc{IntMap}.

While newer versions of the standard allow
for polymorphic types, these still need to be implemented by SMT solvers.
Until the implementations catch up with the standard, feeding operations with
monomorphic types will require Liquid Haskell to be smart about generating
these operations with the appropriate types, instead of putting them in a
preamble once and for all queries.


\section{Evaluation}
\label{SMT-solvers-for-interface-checks}\label{limitations-of-liquid-haskell}

The substitution case study of Section~\ref{capture-avoiding-substitution}
allows for a direct comparison between type methods and refinement type methods. We
can see that the trusted code base of the Liquid Haskell version of
Section~\ref{the-rapier-with-refinement-types} is quite small compared to that
of the foil~\cite{maclaurin23} (reviewed in
Section~\ref{the-rapier-with-stronger-types}). This is in large part
because refinement types can enforce invariants without the need for
abstract types, and such an open interface can be extended by the user.
Contrast with the abstract-type approach where you have to design, upfront, a
set of invariant-preserving operations sufficient to express downstream programs.
None of these functions will benefit from the abstract types invariant, hence
will be part of the trusted code base. Even when we mix refinement and abstract
types as in Section~\ref{ensuring-the-scope-set-is-checked}, we don't have quite
as expensive a trusted code base to consider.

This is not to mean that refinement types are superior to type abstractions.
They are best at enforcing different types of invariants, as discussed in
Section~\ref{ensuring-the-scope-set-is-checked}.

When the invariants of a program naturally involve mathematical objects such as
arithmetic or sets, refinement types are likely to be more approachable,
requiring less careful a design than coming up with an encoding inside and
ML-like type system. On the other hand, when a program needs a theory that Liquid
Haskell, say, doesn't have support for, it may not be that clear and the program
author may need to mobilize comparable effort for refinement types as she would
have for an abstract-type encoding.

The type-checker approach, however, is likely to produce error messages that
are easier both to understand and to fix, provided that the user goal is feasible.
The user is guided into correcting the errors
by the types and the operations of the supporting library. With SMT solvers,
there is always the question of whether a goal is provable or not in the
theories at hand. Is there some additional lemma that is necessary about the user defined
functions? The user has to figure it out on her own. How are the assumptions
insufficient to prove the goal? The user has to compute it on her own too,
although it is plausible that counterexamples or better location information~\cite{webbers24}
can be offered when the tooling matures.

As an example, let us consider the lemma \tc{lemma\-Substitute\-Scopes\-Term} discussed in
Section~\ref{checking-unifyEq}.
If we drop this lemma from the definition of \tc{unifyEq}, we get the following
error message (heavily edited for presentation):
\begin{verbatim}
tests/rapier/Unif.hs:570:7: error:
    Liquid Type Mismatch
    The inferred type
      VV : Term
    is not a subtype of the required type
      VV : {VV : Term |
               IntMapSetInt_isSubsetOf
                 (scopesTerm VV)
                 (IntMap.union (scopesTerm t0) (scopesTerm t1))}
    in the context
      t1 : {t1 : Term |
               consistentSkolemScopesTerm t1
            && unionCommutes (scopesTerm t0) (scopesTerm t1)}

      s : {s : Subst Term |
             isSubsetOf (freeVars (substitute s' t1)) (domain s)}

      t0 : {t0 : Term |
               consistentSkolemScopesTerm t0 && t0 == SA (i, s)}

      s' : {s' : Subst Term |
               inverseSubst (narrowForInvertibility (freeVars t1) s)
                 == Just s'
            && consistentSkolemScopesTerm (substitute s' t1)}

      i : {i : Int | not (Set.member i (skolemSet (substitute s' t1)))}
    Constraint id 3210
    |
570 |     = Just [(i, t')]
    |       ^^^^^^^^^^^^^^...
\end{verbatim}

We can get quickly that the goal is one of the conjuncts in the return
refinement type of \tc{unifyEq}.
But to get at the missing lemma, we need to calculate with the assumptions and the
code of the program. This error message doesn't say what is the term
with the given inferred type, but from the refinement type signature of
\tc{unifyEq} we can deduce that it is referring to \tc{t'}. Substituting \tc{VV}
by \tc{t'} in the predicate of the required type we get
\begin{verbatim}
IntMapSetInt_isSubsetOf
  (scopesTerm t')
  (IntMap.union (scopesTerm t0) (scopesTerm t1))
\end{verbatim}
Inspecting the code we see that \tc{t'} is the result of \tc{substitute s' t1}, so
we substitute again.
\begin{verbatim}
IntMapSetInt_isSubsetOf
  (scopesTerm (substitute s' t1))
  (IntMap.union (scopesTerm t0) (scopesTerm t1))
\end{verbatim}
And now it is time to ponder whether this is true and, if so, why. It turns out that
if the function \tc{substitute} preserves the unification scopes of \tc{t1}, like
in
\begin{verbatim}
scopesTerm t1 = scopesTerm (substitute s' t1)
\end{verbatim}
then the SMT solver can establish that the goal is true. Unfortunately,
\tc{substitute} doesn't preserve unification scopes in general,
but it does preserve the scopes when
the range of the substitution \tc{s'} only contains terms which are variables.

When there are static check failures, insight is often necessary to identify
a missing lemma or a missing precondition.
Recursive functions like \tc{skolemize} or \tc{unify} start with a core
set of conjuncts that is grown as static checks reveal the need of
stronger postconditions for the result of the recursive calls.

Maybe relatedly, the maturity of refinement type checkers in general, and Liquid
Haskell in particular, is rather lacking still. We have encountered a
non-negligible number of bugs and user-experience defects while conducting our
study. Our artifact contains comments explaining the bugs where we were
affected. Thankfully, none of the bugs that we found look really difficult to
fix, but they do have a severe impact on user experience in aggregate.

Besides, Liquid Haskell lacks support for many standard features of Haskell.
In our code we have been using the simplest possible style of programming.
There are no GADTs, no type families, and minimal use of type classes (since
Liquid Haskell has some support for type classes~\cite{liu20}). At the moment,
pushing for more demanding programming patterns is likely to surface more
inconveniences. Aiming for the simplest style is, therefore, a pragmatic constraint of
the current implementation. For further insight on the challenges of using Liquid Haskell,
Gamboa et al.~\cite{gamboa25} report on a study that collects the voices
of its users.

On the performance front, all of the SMT-LIB queries in the unification example run
in 9 seconds, 0.04 seconds for \tc{Subst2.hs}, and 0.03 seconds in \tc{Subst1.hs}.
That is sometimes faster than it takes to compile a module with the GHC compiler.
Where things get slower is when measuring Liquid
Haskell end-to-end, which spends several seconds checking the examples and interacting with the
SMT solver (54 seconds when checking unification, 4 seconds checking \tc{Subst2.hs},
1.5 seconds checking \tc{Subst1.hs}). The authors deem that performance of Liquid Haskell
can be improved to approach that of the SMT solver queries, and probably further by
reducing the amount of queries.

% Liquid Haskell ensures most requirements with little assistance
% because it is delegating much of the work to an underlying SMT solver.
% SMT solvers are tools that decide whether (usually first order) logic formulas are
% satisfiable and provide dedicated mechanisms to reason about various theories
% (sets, strings, arrays, integers, reals, etc).

% In the case of capture-avoiding substitution, multiple queries that
% Liquid Haskell gives to the SMT solver involve reasoning on sets, thus
% making effective use of its capabilities. Moreover, the expression of
% relationships between arguments and result is fairly natural with
% refinement types. When there are lemmas to prove, despite of being
% additional effort to write, they still have proofs that don't require
% a lot of creativity.

% On the other hand, an approach like the foil does need the author to
% think carefully about how to encode the various static checks with the
% type checker, a non trivial supporting library needs to be written,
% and the effort might need further iterations when accounting for
% additional properties.
% We will return to this latter aspect in Section~\ref{SMT-solvers-for-interface-checks}.

% When the property involves user defined recursive functions,
% sometimes it still can be proved automatically if it is attached to the
% refinement type signature of the function. This is the case of the refinement
% types of functions \tc{substitute} and \tc{unification}, where the recursion
% of the proof supports the inductive reasoning required to established the
% property.

% When the property cannot be expressed in the signature of a recursive function,
% like the lemmas that have been presented, writing lemmas becomes necessary. The
% inconvenience is two-fold. In the one hand, the lemma needs to be precisely
% written and then demonstrated. And in the other hand, the user needs to figure
% out the right set of parameters to apply it to at the sites where it is needed.

% These inconveniences are alleviated by the fact that one can still use the
% SMT solvers to write abbreviated demonstrations of the lemmas. Additionally,
% it would be plausible that one can rely on additional mechanisms to help
% discover the parameters to which they are applied. In the case of Liquid
% Haskell, there is an implementation of automatic rewrite rules~\cite{grannan22}
% that is meant to address the problem for lemmas about equalities.

% It would be possible to ask the SMT solver to instantiate the lemma parameters
% as needed by introducing assertions with universal quantification. For instance,
% the universally quantified lemma \tc{lemma\-FreeVars\-Subst\-Union} follows.

% \begin{verbatim}
% (assert
%   (forall
%     ((s (Subst Term))
%      (s1 (Set Int))
%      (s2 (Set Int)))
%     (freeVarsSubst (union s1 s2) s
%        == union (freeVarsSubst s1 s) (freeVarsSubst s2 s))))
% \end{verbatim}

% When checking satisfiability of other assertions, the SMT solver
% will try to use the universally quantified assertion. Unfortunately, the
% behavior of the procedures to find instantiations for quantified variables are
% not easy to predict. This is why Liquid Haskell refrains from doing it~\cite{vazou13}.

Other than using an SMT solver in the fashion of Liquid Haskell, F*~\cite{swamy16},
Why3~\cite{filli13}, or Dafny~\cite{leino17},
the alternatives to implementing interface checks are either to encode the checks in
the type-checker, or to migrate to a dependently typed language for the sake of
static checking~\cite{haftmann10, breitner18, carr22}. Of these alternatives,
we feel like using the type-checker is among the most pragmatic. Maclaurin et al. make
a fine demonstration about the foil.

Perhaps one of the biggest compromises when encoding properties in the type-checker
is that one needs to narrow the expressible properties to a feasible set that allows
to write a supporting library. If we wanted to have static checks like those of the
unification example, we would need new type encodings. Or in other words, new type indices
need to be conceived to relate the parameters of our functions.

\begin{quotation}
$\mathit{skolemize} ::$ $\mathit{Scope}\ s_1 \ldots s_n \rightarrow$ $\mathit{Formula}\ f_1 \ldots f_j \rightarrow$\\
$\mathit{State}\ t_1 \ldots t_k\ (\mathit{Scope}\ e_1 \ldots e_l)\ (\mathit{Formula}\ o_1 \ldots o_m)$
\end{quotation}

Then there would be the effort of writing a library, and later on there would be the
effort of composing the encodings of different libraries when more than one such
is needed. Suppose we started with the static checks to avoid name captures as in
Section~\ref{capture-avoiding-substitution}, and we wanted to add the scopes checks
required to deal with unification variables. With refinement types we only need to add
the corresponding conjuncts to the refinement types.

\begin{verbatim}
type ScopedFormula S = {f:Formula | isSubsetOf (freeVarsFormula f) S}
type ScopedTerm S = {t:Term | isSubsetOf (freeVars t) S}

substituteFormula
  :: scope:Set Int
  -> s:Subst (ScopedTerm scope)
  -> {f:ScopedFormula (domain s) |
           consistentSkolemScopes f
        && UnionCommutes (scopes f) (scopesSubst s)
     }
  -> {v:ScopedFormula scope |
          formulaSize f == formulaSize v
       && consistentSkolemScopes v
       && existsCount v = existsCount f
       && intMapIsSubsetOf (scopes v) (IntMap.union (scopes f) (scopesSubst s))
     }
\end{verbatim}
Besides the usual scope checks, we are checking that the size of the formula
is preserved, that the amount of existential binders is preserved, and that
the unification scopes in the output are those in the input formula and in
the range of the substitution. We also check that substitution preserves
the consistency of the unification scopes.



\section{Conclusions}
\label{conclusions}

The tooling isn't ready for widespread use. Yet it is plausible that in a decently
close future, we have access to SMT solvers and refinement-types to assist us in
our programming.

Refinement types enable a more direct expression of properties,
particularly when the SMT solver supports the relevant theories. Reasoning
mechanisms are reused from the existing tooling, instead of encoding them
in the type checker. This makes easier both to enforce our own invariants and to
compose properties coming from different sources.

The generality of the approach, and the simplicity with which it enables
composition of different properties, are unique features that make it a strong
candidate to impact programming practice in the future.

Through our two case studies, we have tried to make a first step in
understanding how we will be best able to leverage future such tools, even
in situations where we can manage to use current typecheckers today. As a closing
note, let us reproduce the principles that we've proposed throughout the article.

\vspace{-0.8cm}
\listofkeytheorems[
  ignoreall,
  show={principle},
  print-body,
  title={},
  ]

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\appendix

\section{examples/Subst1.hs}
\label{Subst1.hs}
\lstinputlisting{ifl25-lh-src/examples/Subst1.hs}

\section{examples/Subst2.hs}
\label{Subst2.hs}
\lstinputlisting{ifl25-lh-src/examples/Subst2.hs}

\section{examples/Unif.hs}
\label{Unif.hs}
\lstinputlisting{ifl25-lh-src/examples/Unif.hs}

\section{examples/State.hs}
\label{State.hs}
\lstinputlisting{ifl25-lh-src/examples/State.hs}

\section{patches/ifl25-liquidhaskell.patch}
\label{ifl25-liquidhaskell.patch}
\lstinputlisting{ifl25-lh-src/patches/ifl25-liquidhaskell.patch}

\section{patches/ifl25-liquid-fixpoint.patch}
\label{ifl25-liquid-fixpoint.patch}
\lstinputlisting{ifl25-lh-src/patches/ifl25-liquid-fixpoint.patch}

\section{Build instructions for the source code}
\lstinputlisting{ifl25-lh-src/README.md}

\section{System dependencies with nix: shell.nix}
\lstinputlisting{ifl25-lh-src/shell.nix}

\section{Build script: scripts/build.sh}
\lstinputlisting{ifl25-lh-src/scripts/build.sh}

\section{Clean script: scripts/clean.sh}
\lstinputlisting{ifl25-lh-src/scripts/clean.sh}

\section{cabal.project}
\lstinputlisting{ifl25-lh-src/cabal.project}

\section{cabal.project.freeze}
\lstinputlisting{ifl25-lh-src/cabal.project.freeze}

\end{document}

% LocalWords:  invariants axiomatizes axiomatize integrations SMT subtyping
% LocalWords:  equalities GADTs injective skolem postconditon instantiation
% LocalWords:  mergeable codomain

% Local Variables:
% ispell-local-dictionary: "american"
% End:
